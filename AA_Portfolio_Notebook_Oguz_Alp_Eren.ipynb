{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Author:** Oguz Alp Eren\n",
        "\n",
        "Projects in Advanced Machine Learning, Columbia University, 2023"
      ],
      "metadata": {
        "id": "hCCG-dzOjj2u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Projects in Advanced Machine Learning"
      ],
      "metadata": {
        "id": "62GjaSb2jcx_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment #1"
      ],
      "metadata": {
        "id": "2rIkE2rCjV3I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/OguzAlpEren/Oguz-Alp-Eren-Advanced-ML-Projects/blob/0baa9507f378911d1309f6c7e4c36aecc6c110c2/Assignment%20%231/Write%20up%20a%20report%20on%20UN%20World%20Happiness%20Data.ipynb\n"
      ],
      "metadata": {
        "id": "zedCkqaTlrIA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 1 focuses on the analysis of The UN World Happiness Dataset which is a collection of data that measures happiness levels across different countries, using various factors that contribute to overall well-being. The data is compiled by the United Nations and is used to create the annual World Happiness Report, which ranks countries based on their overall happiness scores.\n",
        "\n",
        "An analysis of the relationships between predictor variables and the target variable in the dataset reveals the following correlations:\n",
        "\n",
        "1. Happiness and GDP per capita are positively correlated.\n",
        "2. Happiness and Healthy life expectancy are positively correlated.\n",
        "3. Happiness and Social support are positively correlated.\n",
        "4. Happiness and Freedom to make life choices are positively correlated.\n",
        "\n",
        "Happier individuals tend to have above-average scores in GDP per capita, Healthy life expectancy, Social support, and Freedom to make life choices.\n",
        "\n",
        "To better understand these relationships, six different models were built and tested on the dataset:\n",
        "\n",
        "1. Model 1: Random Forest Classifier with n_estimators=250 and max_depth=4.\n",
        "2. Model 2: GradientBoostingClassifier with n_estimators=45, learning_rate=1.2, and max_depth=1.\n",
        "3. Model 3: Deep Learning (Sequential) with relu activation and hidden layers 64, 32, 16, 16.\n",
        "4. Model 4: Random Forest Classifier with GridSearchCV, cv=10, max_depth=7, and n_estimators=110.\n",
        "5. Model 5: GradientBoostingClassifier with GridSearchCV, cv=10, learning_rate=1.1, max_depth=5, and n_estimators=49.\n",
        "6. Model 6: Deep Learning (Sequential) More Complex, with 128, 64, 64, 64, 32, 32, 16, categorical_crossentropy, optimizer=SGD, metrics=accuracy, batch_size=20, epochs=350, and validation_split=0.25.\n",
        "\n",
        "Feature importances from the Random Forest Classifier indicate that GDP per capita, healthy life expectancy, and social support are important variables in predicting happiness. GDP per capita emerges as a particularly powerful predictor. Region, corruption perceptions, sub-region, and generosity have smaller relationships with the target variable and are considered weaker predictors.\n",
        "\n",
        "This analysis of the UN World Happiness Dataset can provide insights into the factors that contribute to happiness in different countries, informing policymakers and organizations on how to promote well-being and improve quality of life for their citizens."
      ],
      "metadata": {
        "id": "dKYxDoJMMs-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment #2"
      ],
      "metadata": {
        "id": "_ZKPYlDnjYym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/OguzAlpEren/Oguz-Alp-Eren-Advanced-ML-Projects/blob/0baa9507f378911d1309f6c7e4c36aecc6c110c2/Assignment%20%232/Covid%20Positive%20X%20Ray%20image%20data.ipynb"
      ],
      "metadata": {
        "id": "YmdEUFeglxGU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 2 focuses on the analysis of COVID-19 positive X-ray image data, utilizing a dataset created by combining several publicly available databases and images from recent publications. The dataset contains 423 COVID-19 positive chest X-ray images, 1485 viral pneumonia images, and 1579 normal chest X-ray images, covering a diverse range of demographics such as age, gender, and ethnicity.\n",
        "\n",
        "Six distinct models were built and tested on this dataset:\n",
        "\n",
        "1. Model 1: Custom CNN architecture with multiple convolutional layers and max pooling layers.\n",
        "2. Model 2: VGG16 pre-trained model (Transfer Learning).\n",
        "3. Model 3: ResNet50V2 Model (Transfer Learning).\n",
        "4. Model 4: Improved Custom CNN architecture with multiple convolutional layers, max pooling layers, and L2 Regularization.\n",
        "5. Model 5: Improved VGG16 pre-trained model (Transfer Learning).\n",
        "6. Model 6: Improved ResNet50V2 Model (Transfer Learning).\n",
        "\n",
        "Building a predictive model using this dataset can be highly beneficial in the development of AI tools for the rapid and accurate detection of COVID-19 pneumonia from chest X-ray images. The diversity of the dataset and the use of image augmentation techniques improve the generalizability of the models, making them more effective in clinical settings. Researchers can use the dataset to develop and test new AI models, increasing the accuracy and efficiency of COVID-19 diagnosis. This dataset is an essential resource for developing AI-based tools in the fight against COVID-19.\n",
        "\n",
        "Several models were tested with relevant hyperparameters, including increasing the number of filters in convolutional layers, adding L2 regularization, changing the optimizer and learning rate, using transfer learning with pre-trained models, employing global average pooling, specifying a batch size, adding dropout layers, and freezing layers in the base model. All models performed similarly on test scores, with results around 0.71, making it difficult to determine the best performer. However, regularization techniques, dropout layers, and fine-tuning pre-trained layers appear to be effective strategies for improving model performance.\n",
        "\n",
        "The choice between models 5 (Improved VGG16) and 6 (Improved ResNet50V2) could depend on the specific task and dataset. Model 5 may be more suitable for smaller datasets due to fewer trainable parameters and faster training with mixed precision training. In contrast, Model 6 might be more appropriate for larger, more complex datasets, as it has demonstrated better performance in such scenarios. Ultimately, the effectiveness of these models could not be conclusively assessed with just one epoch of training, as models typically require multiple epochs to converge to an optimal solution."
      ],
      "metadata": {
        "id": "d480LsmEMR19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zfdruFvkKyJw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assignment #3"
      ],
      "metadata": {
        "id": "l_f4F3G5jZ10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/OguzAlpEren/Oguz-Alp-Eren-Advanced-ML-Projects/blob/0baa9507f378911d1309f6c7e4c36aecc6c110c2/Assignment%20%233/Text%20Classification%20Using%20the%20Stanford%20SST%20Sentiment%20Dataset.ipynb"
      ],
      "metadata": {
        "id": "91xKYUU9lygG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 3 focuses on the analysis of The Stanford SST Sentiment Dataset which consists of movie reviews that are broken down into phrases and sentences, each labeled with positive, negative, or neutral sentiment. This hierarchical structure allows for a more detailed sentiment analysis across different granularity levels within the text. The dataset is widely used in NLP research, leading to significant advancements in sentiment analysis models.\n",
        "\n",
        "A predictive model trained on the SST dataset has numerous practical applications, such as enabling businesses to monitor customer feedback and make data-driven decisions regarding product development and marketing. Investors can analyze public sentiment about companies to inform their investment strategies. Researchers can utilize the dataset to explore new NLP techniques and refine existing sentiment analysis models. Content creators and media professionals can benefit from understanding audience sentiment to better tailor their content and engage their audience.\n",
        "\n",
        "Six distinct models were built and tested on this dataset:\n",
        "\n",
        "1. Model 1: Simple LSTM model with an Embedding layer and two LSTM layers.\n",
        "2. Model 2: 1D Convolutional Neural Network (CNN) with an Embedding layer, a Conv1D layer, and a GlobalMaxPooling1D layer.\n",
        "3. Model 3: LSTM model with an Embedding layer using pre-trained GloVe word embeddings, an LSTM layer, and a GlobalMaxPooling1D layer.\n",
        "4. Model 4: Bidirectional LSTM model with an Embedding layer, two Bidirectional LSTM layers, and a Dropout layer.\n",
        "5. Model 5: 1D CNN with multiple filter sizes, an Embedding layer, multiple Conv1D layers, a GlobalMaxPooling1D layer, and a Dropout layer.\n",
        "6. Model 6: Bidirectional LSTM model with an Embedding layer using pre-trained GloVe word embeddings, two Bidirectional LSTM layers with dropout and recurrent_dropout, and a GlobalMaxPooling1D layer.\n",
        "\n",
        "Models 3 and 6 outperformed the others with accuracy scores of 0.8057 and 0.8299, respectively. Factors contributing to their success include:\n",
        "\n",
        "- Pre-trained GloVe word embeddings: Both models 3 and 6 utilized GloVe embeddings, which capture semantic relationships from a large corpus of text, improving the models' performance.\n",
        "- LSTM and Bidirectional LSTM layers: Model 3 used a single LSTM layer, while Model 6 employed two Bidirectional LSTM layers, which help capture long-range dependencies in the text. Bidirectional layers in Model 6 were particularly effective as they processed input sequences from both directions.\n",
        "- Dropout and recurrent_dropout: Model 6 incorporated dropout and recurrent_dropout in its Bidirectional LSTM layers, preventing overfitting and improving the model's generalization.\n",
        "- GlobalMaxPooling1D layer: Both models 3 and 6 included a GlobalMaxPooling1D layer to extract the most important features from the input sequence, thereby enhancing the models' performance.\n",
        "\n",
        "The performance of Models 3 and 6 demonstrates the potential of using advanced techniques and pre-trained embeddings for sentiment analysis tasks on the SST dataset."
      ],
      "metadata": {
        "id": "_cYUI0sHKrLE"
      }
    }
  ]
}
