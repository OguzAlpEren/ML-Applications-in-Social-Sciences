{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqHooTSKoFdn"
      },
      "source": [
        "**Author:** Oguz Alp Eren\n",
        "\n",
        "**Course:** Projects in Advanced Machine Learning\n",
        "\n",
        "Columbia University, 2023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxKHjmIaoAgA"
      },
      "source": [
        "#Assignment 3: Text Classification Using the Stanford SST Sentiment Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TN3tbIKqJET"
      },
      "source": [
        "##/Loading the Data and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJuqZXxgqRnW"
      },
      "outputs": [],
      "source": [
        "#install aimodelshare library\n",
        "! pip install aimodelshare==0.0.189"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qv6Nrk_5uGm2",
        "outputId": "ff52e576-9497-46c0-a9c4-3b4c510b08ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Modelshare Username:··········\n",
            "AI Modelshare Password:··········\n",
            "AI Model Share login credentials set successfully.\n"
          ]
        }
      ],
      "source": [
        "#Set credentials using modelshare.org username/password\n",
        "\n",
        "import aimodelshare as ai\n",
        "from aimodelshare.aws import set_credentials\n",
        "    \n",
        "apiurl=\"https://rlxjxnoql9.execute-api.us-east-1.amazonaws.com/prod/m\" #This is the unique rest api that powers this specific Playground\n",
        "\n",
        "set_credentials(apiurl=apiurl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bb3-HdbJuHzU"
      },
      "outputs": [],
      "source": [
        "#Instantiate Competition\n",
        "\n",
        "mycompetition= ai.Competition(apiurl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xL9PRxefqU_e",
        "outputId": "aee8d08b-535c-4469-ce69-1552c349c4a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading [=============================================>   ]\n",
            "\n",
            "Data downloaded successfully.\n"
          ]
        }
      ],
      "source": [
        "# Get competition data\n",
        "from aimodelshare import download_data\n",
        "download_data('public.ecr.aws/y2e2a1d6/sst2_competition_data-repository:latest') "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSsFRVOEqYQ6",
        "outputId": "9dcb1a2d-f098-43a8-f650-4adaabb5ff47"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    The Rock is destined to be the 21st Century 's...\n",
              "1    The gorgeously elaborate continuation of `` Th...\n",
              "2    Singer/composer Bryan Adams contributes a slew...\n",
              "3                 Yet the act is still charming here .\n",
              "4    Whether or not you 're enlightened by any of D...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# Set up X_train, X_test, and y_train_labels objects\n",
        "import pandas as pd\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=Warning)\n",
        "\n",
        "X_train=pd.read_csv(\"sst2_competition_data/X_train.csv\", squeeze=True)\n",
        "X_test=pd.read_csv(\"sst2_competition_data/X_test.csv\", squeeze=True)\n",
        "\n",
        "y_train_labels=pd.read_csv(\"sst2_competition_data/y_train_labels.csv\", squeeze=True)\n",
        "\n",
        "# ohe encode Y data\n",
        "y_train = pd.get_dummies(y_train_labels)\n",
        "\n",
        "X_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHVPmsQ4qevK",
        "outputId": "fad75793-c34e-4bdf-cb52-116b3fe5aa0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6920, 40)\n",
            "(1821, 40)\n"
          ]
        }
      ],
      "source": [
        "# This preprocessor function makes use of the tf.keras tokenizer\n",
        "\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Build vocabulary from training text data\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "\n",
        "# preprocessor tokenizes words and makes sure all documents have the same length\n",
        "def preprocessor(data, maxlen=40, max_words=10000):\n",
        "\n",
        "    sequences = tokenizer.texts_to_sequences(data)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    X = pad_sequences(sequences, maxlen=maxlen)\n",
        "\n",
        "    return X\n",
        "\n",
        "print(preprocessor(X_train).shape)\n",
        "print(preprocessor(X_test).shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DNezkDzxt28s",
        "outputId": "bbba581b-8599-4cab-bb6d-776d8f1aa9d1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your preprocessor is now saved to 'preprocessor.zip'\n"
          ]
        }
      ],
      "source": [
        "ai.export_preprocessor(preprocessor,\"\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMlqjisFLqRT",
        "outputId": "9bba81f7-6ffd-41b0-e588-350e2b38570f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-14 21:22:11--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-04-14 21:22:11--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-04-14 21:22:11--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.2’\n",
            "\n",
            "glove.6B.zip.2      100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
            "\n",
            "2023-04-14 21:24:51 (5.17 MB/s) - ‘glove.6B.zip.2’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "glove_dir = './'\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(os.path.join(glove_dir, 'glove.6B.100d.txt')) as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "embedding_dim = 100\n",
        "word_index = tokenizer.word_index\n",
        "max_words = 10000\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download larger GloVe embeddings\n",
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "glove_dir = './'\n",
        "\n",
        "embeddings_index = {}\n",
        "with open(os.path.join(glove_dir, 'glove.6B.300d.txt')) as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "embedding_dim = 300\n",
        "\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "CFjle1Shupq_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "609b57cd-8ba4-4bec-f378-e08615acaa54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-14 21:27:34--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2023-04-14 21:27:35--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2023-04-14 21:27:35--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip.3’\n",
            "\n",
            "glove.6B.zip.3      100%[===================>] 822.24M  5.02MB/s    in 2m 39s  \n",
            "\n",
            "2023-04-14 21:30:14 (5.18 MB/s) - ‘glove.6B.zip.3’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "replace glove.6B.50d.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-uZrqI7oXwW"
      },
      "source": [
        "##1.) Discuss the dataset in general terms and describe why building a predictive model using this data might be practically useful.  Who could benefit from a model like this? Explain."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SST dataset consists of movie reviews, with each review broken down into phrases and sentences. Sentiment labels are assigned to these components, indicating positive, negative, or neutral emotions. This hierarchical structure enables the analysis of sentiment at multiple levels of granularity within the text. The dataset is an important tool for building and testing sentiment analysis models. Its detailed sentiment labels help create models that understand and interpret complicated opinions in text. As a standard in the NLP field, the SST dataset has led to major improvements in sentiment analysis research and the development of more precise and advanced models.\n",
        "\n",
        "A predictive model trained on the SST dataset can be of practical use in a variety of contexts. Businesses can use sentiment analysis to monitor customer feedback and make informed decisions about product development and marketing strategies. Investors can gauge public sentiment about companies to inform their investment decisions. Researchers can leverage the dataset to further explore NLP techniques and improve upon existing sentiment analysis models. Content creators and media professionals can also benefit from understanding audience sentiment to better tailor their output and engage their viewers or readers. From businesses to researchers and content creators, the applications of a model trained on the SST dataset are vast, making it an essential resource for those seeking to better understand and act on the opinions and emotions expressed in textual data."
      ],
      "metadata": {
        "id": "38e9B58Qs2PN"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-jaV1fsocSa"
      },
      "source": [
        "##2.) Run at least three prediction models to try to predict the SST sentiment dataset well. Submit your best three models to the leader board for the SST Model Share competition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3mMKEI_osZr"
      },
      "source": [
        "###2a.) Model with an Embedding layer and LSTM layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eNVBVyeSU0er"
      },
      "source": [
        "This model is a Keras sequential neural network that consists of an Embedding layer to convert integer-encoded tokens into dense vectors, two LSTM layers to learn and remember long-range dependencies in sequences, and a Dense layer with a softmax activation function for multi-class classification. The model is compiled using the RMSprop optimizer, categorical cross-entropy loss, and accuracy as the evaluation metric. It is trained using preprocessed input data and one-hot encoded labels for 10 epochs with a batch size of 32, while 20% of the data is used for validation purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTu1XwyztDgB",
        "outputId": "94a761c4-af12-4744-d478-afb1ec610cef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 40, 16)            160000    \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 40, 32)            6272      \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 32)                8320      \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 174,658\n",
            "Trainable params: 174,658\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 15s 41ms/step - loss: 0.6336 - acc: 0.6402 - val_loss: 0.6432 - val_acc: 0.7153\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 9s 51ms/step - loss: 0.4401 - acc: 0.7997 - val_loss: 0.6603 - val_acc: 0.6871\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 7s 40ms/step - loss: 0.3305 - acc: 0.8616 - val_loss: 0.6567 - val_acc: 0.6908\n",
            "Epoch 4/10\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.2581 - acc: 0.8918 - val_loss: 0.5271 - val_acc: 0.7818\n",
            "Epoch 5/10\n",
            "173/173 [==============================] - 9s 55ms/step - loss: 0.2074 - acc: 0.9122 - val_loss: 0.6241 - val_acc: 0.7146\n",
            "Epoch 6/10\n",
            "173/173 [==============================] - 6s 34ms/step - loss: 0.1673 - acc: 0.9339 - val_loss: 0.5765 - val_acc: 0.7666\n",
            "Epoch 7/10\n",
            "173/173 [==============================] - 10s 56ms/step - loss: 0.1339 - acc: 0.9482 - val_loss: 0.8404 - val_acc: 0.7204\n",
            "Epoch 8/10\n",
            "173/173 [==============================] - 8s 44ms/step - loss: 0.1061 - acc: 0.9590 - val_loss: 0.9064 - val_acc: 0.7327\n",
            "Epoch 9/10\n",
            "173/173 [==============================] - 6s 35ms/step - loss: 0.0865 - acc: 0.9671 - val_loss: 1.2772 - val_acc: 0.6553\n",
            "Epoch 10/10\n",
            "173/173 [==============================] - 9s 52ms/step - loss: 0.0689 - acc: 0.9760 - val_loss: 0.7196 - val_acc: 0.7962\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding, LSTM\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Build the model\n",
        "model1 = Sequential()\n",
        "model1.add(Embedding(10000, 16, input_length=40))\n",
        "model1.add(LSTM(32, return_sequences=True))\n",
        "model1.add(LSTM(32))\n",
        "model1.add(Dense(2, activation='softmax'))\n",
        "model1.summary()\n",
        "\n",
        "# Compile the model\n",
        "model1.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "# Train the model\n",
        "history = model1.fit(preprocessor(X_train), y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yswzc1g3uCBi"
      },
      "outputs": [],
      "source": [
        "# Save keras model to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model1 = model_to_onnx(model1, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model1.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model1.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22aAYUKmo8K2"
      },
      "source": [
        "###2b.) Model with an Embedding layer and Conv1d layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOkt-7kGU9y-"
      },
      "source": [
        "This model is a Keras sequential neural network that starts with an Embedding layer for converting integer-encoded tokens into dense vectors. Following the Embedding layer, a Conv1D layer with 32 filters and a kernel size of 3 is used to perform convolution operations on the input data, and a ReLU activation function is applied for introducing non-linearity. Afterward, a GlobalMaxPooling1D layer is utilized to reduce the spatial dimensions of the feature maps. The model concludes with a Dense layer containing 2 output units and a softmax activation function for multi-class classification. The model is compiled using the RMSprop optimizer, categorical cross-entropy loss, and accuracy as the evaluation metric. It is trained on preprocessed input data and one-hot encoded labels for 10 epochs with a batch size of 32, using 20% of the data for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0XUMFSO4tVLH",
        "outputId": "bc5251e1-2db7-49fd-eff4-ac412f33421d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 40, 16)            160000    \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 38, 32)            1568      \n",
            "                                                                 \n",
            " global_max_pooling1d (Globa  (None, 32)               0         \n",
            " lMaxPooling1D)                                                  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 161,634\n",
            "Trainable params: 161,634\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 2s 7ms/step - loss: 0.6635 - acc: 0.6093 - val_loss: 0.8551 - val_acc: 0.1582\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 1s 8ms/step - loss: 0.6099 - acc: 0.6638 - val_loss: 0.8052 - val_acc: 0.3461\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 1s 8ms/step - loss: 0.5248 - acc: 0.7354 - val_loss: 0.7319 - val_acc: 0.5043\n",
            "Epoch 4/10\n",
            "173/173 [==============================] - 1s 8ms/step - loss: 0.4231 - acc: 0.8147 - val_loss: 0.6416 - val_acc: 0.6308\n",
            "Epoch 5/10\n",
            "173/173 [==============================] - 1s 8ms/step - loss: 0.3337 - acc: 0.8613 - val_loss: 0.5901 - val_acc: 0.6900\n",
            "Epoch 6/10\n",
            "173/173 [==============================] - 1s 8ms/step - loss: 0.2671 - acc: 0.8958 - val_loss: 0.5688 - val_acc: 0.7132\n",
            "Epoch 7/10\n",
            "173/173 [==============================] - 1s 8ms/step - loss: 0.2210 - acc: 0.9155 - val_loss: 0.5681 - val_acc: 0.7283\n",
            "Epoch 8/10\n",
            "173/173 [==============================] - 1s 8ms/step - loss: 0.1835 - acc: 0.9319 - val_loss: 0.6152 - val_acc: 0.7153\n",
            "Epoch 9/10\n",
            "173/173 [==============================] - 1s 5ms/step - loss: 0.1542 - acc: 0.9426 - val_loss: 0.6304 - val_acc: 0.7312\n",
            "Epoch 10/10\n",
            "173/173 [==============================] - 1s 6ms/step - loss: 0.1300 - acc: 0.9534 - val_loss: 0.7332 - val_acc: 0.7023\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "# Build the model\n",
        "model2 = Sequential()\n",
        "model2.add(Embedding(10000, 16, input_length=40))\n",
        "model2.add(Conv1D(32, 3, activation='relu'))\n",
        "model2.add(GlobalMaxPooling1D())\n",
        "model2.add(Dense(2, activation='softmax'))\n",
        "model2.summary()\n",
        "\n",
        "# Compile the model\n",
        "model2.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "# Train the model\n",
        "history = model2.fit(preprocessor(X_train), y_train,\n",
        "                     epochs=10,\n",
        "                     batch_size=32,\n",
        "                     validation_split=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ns2ZFq7qvZbB"
      },
      "outputs": [],
      "source": [
        "# Save keras model to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model2 = model_to_onnx(model2, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model2.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model2.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJPsGDL2o8du"
      },
      "source": [
        "###2c.) Model with transfer learning with glove embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u72FbytUVB5b"
      },
      "source": [
        "This model is a Keras sequential neural network that employs transfer learning with pre-trained GloVe embeddings. The model starts with an Embedding layer that converts integer-encoded tokens into dense vectors using the GloVe embedding matrix, with the layer's weights set as non-trainable. Next, an LSTM layer with 32 units is used to learn long-range dependencies in sequences and is followed by a GlobalMaxPooling1D layer to reduce the spatial dimensions of the feature maps. Finally, a Dense layer with 2 output units and a softmax activation function is included for multi-class classification. The model is compiled using the RMSprop optimizer, categorical cross-entropy loss, and accuracy as the evaluation metric. It is trained on preprocessed input data and one-hot encoded labels for 10 epochs with a batch size of 32, using 20% of the data for validation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, GlobalMaxPooling1D\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "model3 = Sequential()\n",
        "model3.add(Embedding(max_words, embedding_dim, input_length=40, weights=[embedding_matrix], trainable=False))\n",
        "model3.add(LSTM(32, return_sequences=True))\n",
        "model3.add(GlobalMaxPooling1D())\n",
        "model3.add(Dense(2, activation='softmax'))\n",
        "model3.summary()\n",
        "\n",
        "model3.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "history = model3.fit(preprocessor(X_train), y_train,\n",
        "                     epochs=10,\n",
        "                     batch_size=32,\n",
        "                     validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HihpSI9Q1rwr",
        "outputId": "84091bb2-6146-4ee0-e579-65811efaca98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 40, 300)           3000000   \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 40, 32)            42624     \n",
            "                                                                 \n",
            " global_max_pooling1d_1 (Glo  (None, 32)               0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,042,690\n",
            "Trainable params: 42,690\n",
            "Non-trainable params: 3,000,000\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 10s 44ms/step - loss: 0.5390 - acc: 0.7207 - val_loss: 0.6206 - val_acc: 0.6821\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 5s 28ms/step - loss: 0.4331 - acc: 0.7966 - val_loss: 0.5803 - val_acc: 0.6973\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 4s 24ms/step - loss: 0.3903 - acc: 0.8262 - val_loss: 0.4523 - val_acc: 0.7955\n",
            "Epoch 4/10\n",
            "173/173 [==============================] - 5s 30ms/step - loss: 0.3521 - acc: 0.8414 - val_loss: 0.5189 - val_acc: 0.7464\n",
            "Epoch 5/10\n",
            "173/173 [==============================] - 7s 39ms/step - loss: 0.3239 - acc: 0.8598 - val_loss: 0.6006 - val_acc: 0.7074\n",
            "Epoch 6/10\n",
            "173/173 [==============================] - 4s 26ms/step - loss: 0.2944 - acc: 0.8781 - val_loss: 0.4560 - val_acc: 0.7970\n",
            "Epoch 7/10\n",
            "173/173 [==============================] - 4s 24ms/step - loss: 0.2643 - acc: 0.8934 - val_loss: 0.4669 - val_acc: 0.7832\n",
            "Epoch 8/10\n",
            "173/173 [==============================] - 5s 32ms/step - loss: 0.2397 - acc: 0.9062 - val_loss: 0.5977 - val_acc: 0.7486\n",
            "Epoch 9/10\n",
            "173/173 [==============================] - 6s 37ms/step - loss: 0.2145 - acc: 0.9189 - val_loss: 0.4389 - val_acc: 0.8078\n",
            "Epoch 10/10\n",
            "173/173 [==============================] - 4s 24ms/step - loss: 0.1888 - acc: 0.9324 - val_loss: 0.4362 - val_acc: 0.8143\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQZ39ro3MpZ1"
      },
      "outputs": [],
      "source": [
        "# Save keras model to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model3 = model_to_onnx(model3, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model3.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model3.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCDR_6V1o8wQ"
      },
      "source": [
        "###2d.) Discuss which models performed better and point out relevant hyper-parameter values for successful models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out of the first three models, Model 3 achieved the highest accuracy score of 0.8057, followed by Model 1 with 0.7991, and Model 2 with the lowest score of 0.7749. Model 3 performed the best among the three models, and one of the main reasons behind its success is the use of pre-trained GloVe word embeddings. These embeddings are trained on a large corpus of text and provide a more expressive representation of words. By using these pre-trained embeddings and setting the \"trainable\" parameter to \"False,\" the model takes advantage of the word relationships and semantic information captured in the embeddings.\n",
        "\n",
        "Model 1, which performed slightly worse than Model 3, employed an LSTM architecture without pre-trained embeddings. It used an Embedding layer with a smaller dimension of 16 compared to the 100-dimensional embeddings used in Model 3. This difference in embedding size could have an impact on the model's ability to capture the semantic relationships between words. Despite this, the LSTM layers in Model 1 still helped it achieve a relatively high accuracy score, as LSTM layers are known for their ability to capture long-range dependencies in sequential data. Model 2 had the lowest accuracy score, likely because it used a 1D convolutional neural network (Conv1D) instead of LSTMs. While Conv1D layers can capture local patterns in the input data, they might not be as effective as LSTMs for capturing the long-range dependencies in text data. Furthermore, Model 2 also utilized an Embedding layer with a dimension of 16, which could limit its ability to represent word relationships effectively."
      ],
      "metadata": {
        "id": "dlkwb-mY2Kf9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBv6V2KJojRs"
      },
      "source": [
        "##3.) After you submit your first three models, describe your best model with your team via your team slack channel. Fit and submit up to three more models after learning from your team."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUCGZ_GdOHOs"
      },
      "source": [
        "###3a.) Updated model with an Embedding Layer and LSTM layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjt1fI6rVNSk"
      },
      "source": [
        "This model is a Keras sequential neural network that starts with an Embedding layer for converting integer-encoded tokens into dense vectors of size 128. Following the Embedding layer, a Bidirectional LSTM layer with 128 units is used to learn long-range dependencies in sequences from both forward and backward directions, incorporating dropout and recurrent dropout of 0.2 each. Another Bidirectional LSTM layer with 64 units is added, also with dropout and recurrent dropout set to 0.2. Afterward, a Dropout layer with a rate of 0.5 is used to reduce overfitting. The model concludes with a Dense layer containing 2 output units and a softmax activation function for multi-class classification. The model is compiled using the Adam optimizer with a learning rate of 0.001, categorical cross-entropy loss, and accuracy as the evaluation metric. Early stopping is applied with a monitor on validation loss and a patience of 3 epochs. The model is trained on preprocessed input data and one-hot encoded labels for 10 epochs with a batch size of 32, using 20% of the data for validation, and early stopping as a callback."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dpxx5ohkOGEP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fb0d094-6d77-40e4-ee42-bf7308de8c9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_3 (Embedding)     (None, 40, 128)           1280000   \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 40, 256)          263168    \n",
            " l)                                                              \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 128)              164352    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 128)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,707,778\n",
            "Trainable params: 1,707,778\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 103s 548ms/step - loss: 0.5963 - acc: 0.6814 - val_loss: 0.7000 - val_acc: 0.6604\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 92s 530ms/step - loss: 0.3065 - acc: 0.8766 - val_loss: 0.6885 - val_acc: 0.6655\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 89s 515ms/step - loss: 0.1370 - acc: 0.9530 - val_loss: 0.7146 - val_acc: 0.7435\n",
            "Epoch 4/10\n",
            "173/173 [==============================] - 91s 523ms/step - loss: 0.0589 - acc: 0.9805 - val_loss: 1.0080 - val_acc: 0.7168\n",
            "Epoch 5/10\n",
            "173/173 [==============================] - 95s 549ms/step - loss: 0.0282 - acc: 0.9911 - val_loss: 0.8988 - val_acc: 0.7840\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional, Dropout\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Build the model\n",
        "model4 = Sequential()\n",
        "model4.add(Embedding(10000, 128, input_length=40))\n",
        "model4.add(Bidirectional(LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
        "model4.add(Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)))\n",
        "model4.add(Dropout(0.5))\n",
        "model4.add(Dense(2, activation='softmax'))\n",
        "model4.summary()\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(lr=0.001)\n",
        "model4.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "# Set early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model4.fit(preprocessor(X_train), y_train,\n",
        "                     epochs=10,\n",
        "                     batch_size=32,\n",
        "                     validation_split=0.2,\n",
        "                     callbacks=[early_stopping])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQAWpBo5O2Wf"
      },
      "outputs": [],
      "source": [
        "# Save keras model to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model4 = model_to_onnx(model4, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model4.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model4.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-62a-DnPGS5"
      },
      "source": [
        "###3b.) Updated model with an Embedding layer and Conv1d layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUZHj2ufVTtN"
      },
      "source": [
        "This model employs a Keras functional API to create a neural network with an Input layer taking sequences of length 40. An Embedding layer follows, transforming integer-encoded tokens into dense vectors of size 128. Next, three Conv1D layers with 128 filters and varying kernel sizes (3, 4, and 5) are applied to the embedding output to capture local patterns of different lengths. The outputs of these convolutional layers are concatenated along the time axis, resulting in a single feature map. A GlobalMaxPooling1D layer is used to extract the most important features from the concatenated feature map. A Dropout layer with a rate of 0.5 helps reduce overfitting, and a Dense output layer with 2 units and a softmax activation function is used for multi-class classification. The model is compiled with the Adam optimizer at a learning rate of 0.001, categorical cross-entropy loss, and accuracy as the evaluation metric. Early stopping is applied with a monitor on validation loss and a patience of 3 epochs. The model is trained on preprocessed input data and one-hot encoded labels for 10 epochs with a batch size of 32, using 20% of the data for validation, and early stopping as a callback."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9m-w3gUxQgQ2",
        "outputId": "6626f002-8a0e-428a-8ea7-b6ac8942c2a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 40)]         0           []                               \n",
            "                                                                                                  \n",
            " embedding_4 (Embedding)        (None, 40, 128)      1280000     ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " conv1d_1 (Conv1D)              (None, 38, 128)      49280       ['embedding_4[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_2 (Conv1D)              (None, 37, 128)      65664       ['embedding_4[0][0]']            \n",
            "                                                                                                  \n",
            " conv1d_3 (Conv1D)              (None, 36, 128)      82048       ['embedding_4[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate (Concatenate)      (None, 111, 128)     0           ['conv1d_1[0][0]',               \n",
            "                                                                  'conv1d_2[0][0]',               \n",
            "                                                                  'conv1d_3[0][0]']               \n",
            "                                                                                                  \n",
            " global_max_pooling1d_2 (Global  (None, 128)         0           ['concatenate[0][0]']            \n",
            " MaxPooling1D)                                                                                    \n",
            "                                                                                                  \n",
            " dropout_1 (Dropout)            (None, 128)          0           ['global_max_pooling1d_2[0][0]'] \n",
            "                                                                                                  \n",
            " dense_4 (Dense)                (None, 2)            258         ['dropout_1[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 1,477,250\n",
            "Trainable params: 1,477,250\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 12s 64ms/step - loss: 0.6448 - acc: 0.6302 - val_loss: 0.7928 - val_acc: 0.3714\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 13s 77ms/step - loss: 0.4310 - acc: 0.8111 - val_loss: 0.5646 - val_acc: 0.7319\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 14s 78ms/step - loss: 0.2035 - acc: 0.9236 - val_loss: 0.6190 - val_acc: 0.7355\n",
            "Epoch 4/10\n",
            "173/173 [==============================] - 11s 62ms/step - loss: 0.0861 - acc: 0.9733 - val_loss: 0.8710 - val_acc: 0.7023\n",
            "Epoch 5/10\n",
            "173/173 [==============================] - 14s 81ms/step - loss: 0.0345 - acc: 0.9922 - val_loss: 1.0338 - val_acc: 0.7001\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding, Conv1D, GlobalMaxPooling1D, Dropout, Input, concatenate\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Build the model\n",
        "input_layer = Input(shape=(40,))\n",
        "embedding_layer = Embedding(10000, 128, input_length=40)(input_layer)\n",
        "\n",
        "conv1 = Conv1D(128, 3, activation='relu')(embedding_layer)\n",
        "conv2 = Conv1D(128, 4, activation='relu')(embedding_layer)\n",
        "conv3 = Conv1D(128, 5, activation='relu')(embedding_layer)\n",
        "\n",
        "merged = concatenate([conv1, conv2, conv3], axis=1)\n",
        "pooling = GlobalMaxPooling1D()(merged)\n",
        "dropout = Dropout(0.5)(pooling)\n",
        "output_layer = Dense(2, activation='softmax')(dropout)\n",
        "\n",
        "model5 = Model(inputs=input_layer, outputs=output_layer)\n",
        "model5.summary()\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(lr=0.001)\n",
        "model5.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "# Set early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model5.fit(preprocessor(X_train), y_train,\n",
        "                     epochs=10,\n",
        "                     batch_size=32,\n",
        "                     validation_split=0.2,\n",
        "                     callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEA8scd7RhCP"
      },
      "outputs": [],
      "source": [
        "# Save keras model to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model5 = model_to_onnx(model5, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model5.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model5.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D20TOtwlPobA"
      },
      "source": [
        "###3c.) Updated model with transfer learning with glove embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-K1-LsUVk9l"
      },
      "source": [
        "This model is a sequential neural network that begins with an Embedding layer that uses pre-trained GloVe embeddings. The embedding layer has a vocabulary size of max_words, an embedding dimension equal to the pre-trained GloVe embeddings, and an input length of 40. The Embedding layer is followed by two Bidirectional LSTM layers with 128 and 64 units, respectively. Both LSTM layers use dropout and recurrent dropout rates of 0.2 to reduce overfitting and return sequences to feed the next layer. A GlobalMaxPooling1D layer is employed to extract the most important features from the sequences. A Dropout layer with a rate of 0.5 is added to further prevent overfitting, and a Dense output layer with 2 units and a softmax activation function performs multi-class classification. The model is compiled using the Adam optimizer with a learning rate of 0.001, a categorical cross-entropy loss function, and accuracy as the evaluation metric. Early stopping with a patience of 3 epochs is implemented to prevent overfitting. The model is trained for 10 epochs using a batch size of 32 and a validation split of 0.2.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgFE4a-ARqek",
        "outputId": "b2812939-e81e-4930-f23f-91cbad4b1975"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_5 (Embedding)     (None, 40, 300)           3000000   \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 40, 256)          439296    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 40, 128)          164352    \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " global_max_pooling1d_3 (Glo  (None, 128)              0         \n",
            " balMaxPooling1D)                                                \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 2)                 258       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,603,906\n",
            "Trainable params: 3,603,906\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "173/173 [==============================] - 132s 699ms/step - loss: 0.5183 - acc: 0.7370 - val_loss: 0.5109 - val_acc: 0.7637\n",
            "Epoch 2/10\n",
            "173/173 [==============================] - 116s 670ms/step - loss: 0.3331 - acc: 0.8553 - val_loss: 0.4542 - val_acc: 0.8056\n",
            "Epoch 3/10\n",
            "173/173 [==============================] - 116s 672ms/step - loss: 0.2007 - acc: 0.9220 - val_loss: 0.5189 - val_acc: 0.7760\n",
            "Epoch 4/10\n",
            "173/173 [==============================] - 120s 693ms/step - loss: 0.1179 - acc: 0.9554 - val_loss: 0.8189 - val_acc: 0.7370\n",
            "Epoch 5/10\n",
            "173/173 [==============================] - 116s 671ms/step - loss: 0.0662 - acc: 0.9751 - val_loss: 0.6979 - val_acc: 0.8020\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, GlobalMaxPooling1D, Dropout, Bidirectional\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Build the model\n",
        "model6 = Sequential()\n",
        "model6.add(Embedding(max_words, embedding_dim, input_length=40, weights=[embedding_matrix], trainable=True))\n",
        "model6.add(Bidirectional(LSTM(128, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
        "model6.add(Bidirectional(LSTM(64, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)))\n",
        "model6.add(GlobalMaxPooling1D())\n",
        "model6.add(Dropout(0.5))\n",
        "model6.add(Dense(2, activation='softmax'))\n",
        "model6.summary()\n",
        "\n",
        "# Compile the model\n",
        "optimizer = Adam(lr=0.001)\n",
        "model6.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['acc'])\n",
        "\n",
        "# Set early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "history = model6.fit(preprocessor(X_train), y_train,\n",
        "                     epochs=10,\n",
        "                     batch_size=32,\n",
        "                     validation_split=0.2,\n",
        "                     callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTAOwtwGSZbJ"
      },
      "outputs": [],
      "source": [
        "# Save keras model to local ONNX file\n",
        "from aimodelshare.aimsonnx import model_to_onnx\n",
        "\n",
        "onnx_model6 = model_to_onnx(model6, framework='keras',\n",
        "                          transfer_learning=False,\n",
        "                          deep_learning=True)\n",
        "\n",
        "with open(\"model6.onnx\", \"wb\") as f:\n",
        "    f.write(onnx_model6.SerializeToString())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yStOwqSspcQn"
      },
      "source": [
        "###3d.) Discuss results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Among models 4-6, Model 6 achieved the highest accuracy score of 0.8299, followed by Model 5 with 0.7925, and Model 4 with the lowest score of 0.7804.Model 6 performed the best, and one key factor behind its success is the use of pre-trained GloVe word embeddings, similar to Model 3. These pre-trained embeddings provide a richer representation of words by capturing semantic relationships from a large corpus of text. Another important aspect of Model 6 is its use of bidirectional LSTMs with dropout and recurrent_dropout for regularization. Bidirectional LSTMs allow the model to process the input sequence from both directions, which can help capture more context and improve the model's ability to understand the underlying patterns in the data.\n",
        "\n",
        "Model 5, which had a slightly lower accuracy than Model 6, used a combination of 1D convolutional neural networks (Conv1D) with varying filter sizes and a GlobalMaxPooling1D layer to extract features from the input data. The concatenated output of these Conv1D layers is then passed through a Dropout layer for regularization. Although this architecture can capture local patterns in the data, it may not be as effective as bidirectional LSTMs in capturing long-range dependencies in text data. Model 4 had the lowest accuracy among the three models. Like Model 6, it utilized bidirectional LSTMs and dropout for regularization. However, one key difference between Model 4 and Model 6 is the absence of pre-trained GloVe embeddings in Model 4. Instead, it used an Embedding layer with a higher dimension of 128 but learned the embeddings from scratch. This may have limited the model's ability to capture semantic relationships effectively, leading to a lower performance compared to Model 6."
      ],
      "metadata": {
        "id": "vo0sDnOq3b9O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXOKk7eFuOCw",
        "outputId": "71f6c95c-b1b5-4ad6-c770-a21c9c44ac2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 2s 15ms/step\n",
            "Insert search tags to help users find your model (optional): emin\n",
            "Provide any useful notes about your model (optional): emine\n",
            "\n",
            "Your model has been submitted as model version 138\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ],
      "source": [
        "#Submit Model 1: \n",
        "\n",
        "#-- Generate predicted y values (Model 1)\n",
        "#Note: Keras predict returns the predicted column index location for classification models\n",
        "prediction_column_index=model1.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "\n",
        "# extract correct prediction labels \n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "# Submit Model 1 to Competition Leaderboard\n",
        "mycompetition.submit_model(model_filepath = \"model1.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-ZacgQSvOvo",
        "outputId": "82d576a8-f986-4468-e9b5-f0750c82e6a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 0s 2ms/step\n",
            "Insert search tags to help users find your model (optional): emin\n",
            "Provide any useful notes about your model (optional): emine\n",
            "\n",
            "Your model has been submitted as model version 139\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ],
      "source": [
        "#Submit Model 2: \n",
        "\n",
        "#-- Generate predicted y values (Model 2)\n",
        "#Note: Keras predict returns the predicted column index location for classification models\n",
        "prediction_column_index=model2.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "\n",
        "# extract correct prediction labels \n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "# Submit Model 2 to Competition Leaderboard\n",
        "mycompetition.submit_model(model_filepath = \"model2.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPIRoQptM0qN",
        "outputId": "953acc04-4f66-4e31-f706-490526730c82"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 1s 12ms/step\n",
            "Insert search tags to help users find your model (optional): emin\n",
            "Provide any useful notes about your model (optional): emine\n",
            "\n",
            "Your model has been submitted as model version 140\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ],
      "source": [
        "#Submit Model 3: \n",
        "\n",
        "#-- Generate predicted y values (Model 3)\n",
        "#Note: Keras predict returns the predicted column index location for classification models\n",
        "prediction_column_index=model3.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "\n",
        "# extract correct prediction labels \n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "# Submit Model 3 to Competition Leaderboard\n",
        "mycompetition.submit_model(model_filepath = \"model3.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IC6tLQ6PCPb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d66ed97-7ff8-44ec-c495-5a300c1d758b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 4s 59ms/step\n",
            "Insert search tags to help users find your model (optional): emin\n",
            "Provide any useful notes about your model (optional): emine\n",
            "\n",
            "Your model has been submitted as model version 141\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ],
      "source": [
        "#Submit Model 4: \n",
        "\n",
        "#-- Generate predicted y values (Model 4)\n",
        "#Note: Keras predict returns the predicted column index location for classification models\n",
        "prediction_column_index=model4.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "\n",
        "# extract correct prediction labels \n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "# Submit Model 4 to Competition Leaderboard\n",
        "mycompetition.submit_model(model_filepath = \"model4.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A4sNskEuRmPf",
        "outputId": "4a85f6a1-202c-479c-f717-32b085071b93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 1s 13ms/step\n",
            "Insert search tags to help users find your model (optional): emin\n",
            "Provide any useful notes about your model (optional): emine\n",
            "\n",
            "Your model has been submitted as model version 142\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ],
      "source": [
        "#Submit Model 5: \n",
        "\n",
        "#-- Generate predicted y values (Model 5)\n",
        "#Note: Keras predict returns the predicted column index location for classification models\n",
        "prediction_column_index=model5.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "\n",
        "# extract correct prediction labels \n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "# Submit Model 5 to Competition Leaderboard\n",
        "mycompetition.submit_model(model_filepath = \"model5.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5DlxkdN9SdtK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f642611-b398-49db-dfea-b4d76562ee0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57/57 [==============================] - 6s 85ms/step\n",
            "Insert search tags to help users find your model (optional): emin\n",
            "Provide any useful notes about your model (optional): emine\n",
            "\n",
            "Your model has been submitted as model version 143\n",
            "\n",
            "To submit code used to create this model or to view current leaderboard navigate to Model Playground: \n",
            "\n",
            " https://www.modelshare.org/detail/model:2763\n"
          ]
        }
      ],
      "source": [
        "#Submit Model 6: \n",
        "\n",
        "#-- Generate predicted y values (Model 6)\n",
        "#Note: Keras predict returns the predicted column index location for classification models\n",
        "prediction_column_index=model6.predict(preprocessor(X_test)).argmax(axis=1)\n",
        "\n",
        "# extract correct prediction labels \n",
        "prediction_labels = [y_train.columns[i] for i in prediction_column_index]\n",
        "\n",
        "# Submit Model 6 to Competition Leaderboard\n",
        "mycompetition.submit_model(model_filepath = \"model6.onnx\",\n",
        "                                 preprocessor_filepath=\"preprocessor.zip\",\n",
        "                                 prediction_submission=prediction_labels)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get leaderboard\n",
        "\n",
        "data = mycompetition.get_leaderboard()\n",
        "mycompetition.stylize_leaderboard(data)"
      ],
      "metadata": {
        "id": "xN5iXMuz18UC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KCpTsPVkooKY"
      },
      "source": [
        "##4.) Discuss which models you tried and which models performed better and point out relevant hyper-parameter values for successful models."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model 1: A simple LSTM model with an Embedding layer and two LSTM layers.\n",
        "Model 2: A 1D Convolutional Neural Network with an Embedding layer, a Conv1D layer, and a GlobalMaxPooling1D layer.\n",
        "Model 3: An LSTM model with an Embedding layer using pre-trained GloVe word embeddings, an LSTM layer, and a GlobalMaxPooling1D layer.\n",
        "Model 4: A Bidirectional LSTM model with an Embedding layer, two Bidirectional LSTM layers, and a Dropout layer.\n",
        "Model 5: A 1D Convolutional Neural Network with multiple filter sizes, an Embedding layer, multiple Conv1D layers, a GlobalMaxPooling1D layer, and a Dropout layer.\n",
        "Model 6: A Bidirectional LSTM model with an Embedding layer using pre-trained GloVe word embeddings, two Bidirectional LSTM layers with dropout and recurrent_dropout, and a GlobalMaxPooling1D layer.\n",
        "\n",
        "Model 3 and Model 6 performed the best among all models with accuracy scores of 0.8057 and 0.8299, respectively. The key factors contributing to their success include:\n",
        "\n",
        "- Pre-trained GloVe word embeddings: Both models utilized pre-trained GloVe embeddings, which capture semantic relationships from a large corpus of text, enriching the word representations and improving the model's performance.\n",
        "\n",
        "- LSTM and Bidirectional LSTM layers: Model 3 employed a single LSTM layer, while Model 6 used two Bidirectional LSTM layers. These layers help capture long-range dependencies in text data, with bidirectional layers in Model 6 being particularly effective by processing input sequences from both directions.\n",
        "\n",
        "- Dropout and recurrent_dropout: Model 6 used dropout and recurrent_dropout for regularization in the Bidirectional LSTM layers, preventing overfitting and improving generalization.\n",
        "\n",
        "- GlobalMaxPooling1D layer: Both models used a GlobalMaxPooling1D layer to extract the most important features from the input sequence, which is beneficial for the model's performance."
      ],
      "metadata": {
        "id": "DDLkQmt74RyH"
      }
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
